{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yTDyciGInd4V"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.preprocessing import Normalizer,StandardScaler , PowerTransformer, RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer#IterativeImputer\n",
    "from category_encoders import TargetEncoder, BinaryEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, SelectKBest\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV, KFold\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV, HalvingRandomSearchCV\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn import set_config\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, StackingClassifier, StackingRegressor, VotingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "\n",
    "import optuna\n",
    "from optuna.distributions import CategoricalDistribution, IntDistribution, FloatDistribution\n",
    "from optuna.integration import OptunaSearchCV, ShapleyImportanceEvaluator\n",
    "from optuna.visualization import plot_param_importances\n",
    "import re\n",
    "import feature_engine as fe\n",
    "import category_encoders as ce\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "nDM0gUrHnd4e",
    "outputId": "ad2bcd32-cb9f-470b-d2ee-d8ace42b60a3"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "park_df = pd.read_csv('data/park.csv')\n",
    "care_df = pd.read_csv('data/day_care_center.csv')\n",
    "submission_df = pd.read_csv('data/submission.csv')\n",
    "\n",
    "\n",
    "se_subway = pd.read_csv('data/서울지하철.csv', encoding = 'cp949')\n",
    "pu_subway = pd.read_csv('data/부산지하철.csv', encoding = 'cp949')\n",
    "pu_hos = pd.read_csv('data/부산병원.csv', encoding = 'cp949')\n",
    "se_hos = pd.read_csv('data/서울병원.csv', encoding = 'cp949')\n",
    "pu_aca = pd.read_csv('data/부산학원.csv', encoding = 'cp949')\n",
    "pu_gyo = pd.read_csv('data/부산교습소.csv', encoding = 'cp949')\n",
    "se_aca = pd.read_csv('data/서울학원.csv', encoding = 'cp949')\n",
    "phar = pd.read_csv('data/전국약국.csv', encoding = 'cp949')\n",
    "\n",
    "se_mar = pd.read_csv('data/상권정보_서울.csv', encoding = 'utf-8')\n",
    "pu_mar = pd.read_csv('data/상권정보_부산.csv', encoding = 'utf-8')\n",
    "care_se = pd.read_csv('data/서울시 어린이집 정보(표준 데이터).csv', encoding = 'cp949')\n",
    "care_pu = pd.read_csv('data/부산어린이집.csv', encoding = 'cp949')\n",
    "sungdong = pd.read_csv('data/성동구.csv', encoding = 'cp949')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0c7z3pxnd4i"
   },
   "source": [
    "# 1. 추가데이터, 기존 데이터에 대한 생성 및 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7RjdgR2nd4l"
   },
   "source": [
    "    Train, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364966"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hckI0H5nnd4m"
   },
   "outputs": [],
   "source": [
    "test_id = test_df.transaction_id\n",
    "\n",
    "\n",
    "train_df = train_df.drop(['transaction_id','apartment_id','addr_kr', 'jibun', 'apt'],axis=1)\n",
    "test_df = test_df.drop([ 'apartment_id','addr_kr', 'jibun', 'apt'],axis=1)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "train_df.loc[train_df['transaction_date'] == '1~10', 'transaction_date'] = 1\n",
    "train_df.loc[train_df['transaction_date'] == '11~20', 'transaction_date'] = 2\n",
    "train_df.loc[train_df['transaction_date'] != int, 'transaction_date'] = 3\n",
    "\n",
    "test_df.loc[test_df['transaction_date'] == '1~10', 'transaction_date'] = 1\n",
    "test_df.loc[test_df['transaction_date'] == '11~20', 'transaction_date'] = 2\n",
    "test_df.loc[test_df['transaction_date'] != int, 'transaction_date'] = 3\n",
    "\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MS7nAU69nd4n"
   },
   "source": [
    "    역세권"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "j5NQLBmpnd4o"
   },
   "outputs": [],
   "source": [
    "## 서울 ##\n",
    "se_subway['dong'] = se_subway['도로명주소'].str.extract(r'\\((.*?)\\)', expand=False) # 주소 혼용 데이터에서 동만 추출\n",
    "\n",
    "dong_line_counts = se_subway.groupby(['dong', '호선']).size().reset_index(name='count') # 동별 호선별 역의 개수\n",
    "se_subway = pd.merge(se_subway, dong_line_counts, on=['dong', '호선'], how='left')# se_subway에 병합\n",
    "\n",
    "columns_to_drop = ['역명', '구주소', '도로명주소', '전화번호']\n",
    "se_subway = se_subway.drop(columns=columns_to_drop)\n",
    "\n",
    "## 부산 ##\n",
    "pu_subway['dong'] = pu_subway['지번주소'].str.extract(r'\\b(\\w+)동\\b', expand=False)# 주소 혼용 데이터에서 동만 추출\n",
    "pu_subway['dong'] = pu_subway['dong'] + '동'\n",
    "pu_subway.loc[4, 'dong'] = '남포동1가'\n",
    "pu_subway.loc[11, 'dong'] = '동대신동2가'\n",
    "pu_subway.loc[24, 'dong'] = '서대신동2가'\n",
    "pu_subway.loc[32, 'dong'] = '남포동6가'\n",
    "pu_subway.loc[36, 'dong'] = '중앙동4가'\n",
    "pu_subway.loc[38, 'dong'] = '토성동3가'\n",
    "pu_subway.loc[50, 'dong'] = '물금읍 범어리'\n",
    "pu_subway.loc[65, 'dong'] = '물금읍 범어리'\n",
    "pu_subway.loc[78, 'dong'] = '물금읍 증산리'\n",
    "pu_subway.loc[81, 'dong'] = '동면 가산리'\n",
    "pu_subway.loc[100, 'dong'] = '철마면 고촌리'\n",
    "pu_subway.loc[110, 'dong'] = '철마면 안평리' # 결측값 가내수공업\n",
    "\n",
    "dong_line_counts = pu_subway.groupby(['선명', 'dong']).size().reset_index(name='count')# 동별 호선별 역의 개수\n",
    "pu_subway = pd.merge(pu_subway, dong_line_counts, on=['dong', '선명'], how='left')# pu_subway에 병합\n",
    "\n",
    "columns_to_drop = ['철도운영기관명', '역명', '지번주소', '도로명주소']\n",
    "pu_subway = pu_subway.drop(columns=columns_to_drop)\n",
    "pu_subway = pu_subway.rename(columns={'선명': '호선'})\n",
    "\n",
    "##지하철 전체병합##\n",
    "subway_total = pd.concat([se_subway, pu_subway], axis=0, ignore_index=True)\n",
    "subway_total.drop_duplicates(subset=['호선', 'dong', 'count'], inplace=True)\n",
    "\n",
    "## train,test에 병합 ##\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364966"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_df, subway_total[['dong', '호선', 'count']], on='dong', how='left')\n",
    "test_df = pd.merge(test_df, subway_total[['dong', '호선', 'count']], on='dong', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop_duplicates(subset='transaction_id', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop([ 'transaction_id'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7UFEN-Nnd4q"
   },
   "source": [
    "    병원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FHKMLBxNnd4r"
   },
   "outputs": [],
   "source": [
    "## 부산 ##\n",
    "pu_hos = pu_hos[pu_hos['영업상태명'] != '폐업']\n",
    "pu_hos = pu_hos[pu_hos['영업상태명'] != '취소/말소/만료/정지/중지']\n",
    "pu_hos = pu_hos[pu_hos['영업상태명'] != '휴업']\n",
    "\n",
    "pu_hos.drop(pu_hos.columns[0:19], axis=1, inplace=True)\n",
    "pu_hos.drop(pu_hos.columns[1:], axis=1, inplace=True)\n",
    "\n",
    "pu_hos['dong'] = pu_hos['도로명전체주소'].str.extract(r'\\((.*?)\\)', expand=False)\n",
    "pu_hos['dong'] = pu_hos['dong'].str.split(',').str[0]\n",
    "pu_hos = pu_hos.rename(columns={'도로명전체주소': '주소'})\n",
    "\n",
    "## 서울 ##\n",
    "se_hos.drop(se_hos.columns[0:1], axis=1, inplace=True)\n",
    "se_hos.drop(se_hos.columns[1:], axis=1, inplace=True)\n",
    "\n",
    "se_hos['dong'] = se_hos['주소'].str.extract(r'\\((.*?)\\)', expand=False)\n",
    "se_hos['dong'] = se_hos['dong'].str.split(',').str[0]\n",
    "\n",
    "## 병원 전체병합 ##\n",
    "hos_total = pd.concat([se_hos, pu_hos], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "## train, test에 병합 ##\n",
    "hos_counts_dict = hos_total['dong'].value_counts().to_dict()\n",
    "train_df['hos_dongs_count'] = train_df['dong'].map(hos_counts_dict)\n",
    "test_df['hos_dongs_count'] = test_df['dong'].map(hos_counts_dict)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-nZrY9Pnd4t"
   },
   "source": [
    "    학원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6Wn5FVFond4u"
   },
   "outputs": [],
   "source": [
    "pu_gyo = pu_gyo.rename(columns={'교습소명': '학원명'}) # 컬럼명 바꾸기\n",
    "\n",
    "pu_total = pd.concat([pu_aca, pu_gyo], axis=0, ignore_index=True) # 과목별 학원 병합\n",
    "pu_total.drop(pu_total.columns[0:4], axis=1, inplace=True)\n",
    "pu_total.drop(pu_total.columns[1:], axis=1, inplace=True)\n",
    "pu_total['dong'] = pu_total['위치'].str.extract(r'\\((.*?)\\)', expand=False)\n",
    "pu_total['dong'] = pu_total['dong'].str.split(',').str[0]\n",
    "\n",
    "\n",
    "se_aca.drop(se_aca.columns[0:5], axis=1, inplace=True)\n",
    "se_aca.drop(se_aca.columns[1:], axis=1, inplace=True)\n",
    "se_aca['dong'] = se_aca['도로명상세주소'].str.extract(r'\\((.*?)\\)', expand=False)\n",
    "se_aca['dong'] = se_aca['dong'].str.split(',').str[0]\n",
    "se_aca = se_aca.rename(columns={'도로명상세주소': '위치'})\n",
    "\n",
    "aca_total = pd.concat([pu_total, se_aca], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "## train, test에 병합 ##\n",
    "aca_counts_dict = aca_total['dong'].value_counts().to_dict()\n",
    "train_df['aca_dongs_count'] = train_df['dong'].map(aca_counts_dict)\n",
    "test_df['aca_dongs_count'] = test_df['dong'].map(aca_counts_dict)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaHse5eInd4v"
   },
   "source": [
    "    공원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Sy9YnDHond4v"
   },
   "outputs": [],
   "source": [
    "park_df.at[565,'gu'] = '중구'\n",
    "park_df.at[566,'gu'] = '부산진구'\n",
    "park_df.at[567,'gu'] = '동래구'\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# 'transaction_year_month' 열의 고유한 값들을 오름차순으로 정렬\n",
    "sorted_unique_values = sorted(train_df['transaction_year_month'].unique())\n",
    "mapping_dict = {value: index + 1 for index, value in enumerate(sorted_unique_values)}\n",
    "train_df['transaction_year_month_sort'] = train_df['transaction_year_month'].map(mapping_dict)\n",
    "\n",
    "sorted_unique_values = sorted(test_df['transaction_year_month'].unique())\n",
    "mapping_dict = {value: index + 1 for index, value in enumerate(sorted_unique_values)}\n",
    "test_df['transaction_year_month_sort'] = test_df['transaction_year_month'].map(mapping_dict)\n",
    "\n",
    "# 해당작업 후 제거\n",
    "test_df = test_df.drop(['transaction_year_month'],axis=1)\n",
    "train_df = train_df.drop(['transaction_year_month'],axis=1)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "\n",
    "#care_df에는 동대문구, 용산구가 없다.\n",
    "\n",
    "park_care_df = pd.merge(park_df, care_df, on='gu', how='left')\n",
    "\n",
    "train_df['dong_park_count'] = train_df['dong'].map(park_care_df.groupby('dong')['park_name'].count())\n",
    "test_df['dong_park_count'] = test_df['dong'].map(park_care_df.groupby('dong')['park_name'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBH7eQhjnd4x"
   },
   "source": [
    "    약국"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AnuA-f1Ynd4x"
   },
   "outputs": [],
   "source": [
    "phar.drop(phar.columns[0:5], axis=1, inplace=True)\n",
    "phar.drop(phar.columns[4:], axis=1, inplace=True)\n",
    "\n",
    "시도코드명 = ['서울', '부산']\n",
    "phar = phar[phar['시도코드명'].isin(시도코드명)]\n",
    "\n",
    "phar.dropna(subset=['읍면동'], inplace=True)\n",
    "\n",
    "## train, test에 병합 ##\n",
    "phar_counts_dict = phar['읍면동'].value_counts().to_dict()\n",
    "train_df['phar_counts_count'] = train_df['dong'].map(phar_counts_dict)\n",
    "test_df['phar_counts_count'] = test_df['dong'].map(phar_counts_dict)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyxRbKRFnd4y"
   },
   "source": [
    "    상권"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uhf3hgxnnd4z"
   },
   "outputs": [],
   "source": [
    "mar_total = pd.concat([se_mar, pu_mar], axis=0, ignore_index=True)\n",
    "\n",
    "## train, test에 병합 ##\n",
    "mar_counts_dict = mar_total['법정동명'].value_counts().to_dict()\n",
    "train_df['mar_counts_count'] = train_df['dong'].map(mar_counts_dict)\n",
    "test_df['mar_counts_count'] = test_df['dong'].map(mar_counts_dict)\n",
    "\n",
    "major_industry_by_dong = mar_total.groupby('법정동명')['상권업종대분류명'].agg(lambda x: x.value_counts().idxmax())\n",
    "train_df['major_industry'] = train_df['dong'].map(major_industry_by_dong)\n",
    "test_df['major_industry'] = test_df['dong'].map(major_industry_by_dong)\n",
    "###############################################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbocGCBtnd40"
   },
   "source": [
    "    care"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Sz8bNADxnd40"
   },
   "outputs": [],
   "source": [
    "care_se = care_se.iloc[:, :7]\n",
    "columns_to_drop = ['어린이집코드', '운영현황', '우편번호']\n",
    "care_se = care_se.drop(columns=columns_to_drop)\n",
    "care_se = care_se.rename(columns={'시군구명': 'gu', '어린이집명': 'day_care_name', '어린이집유형': 'day_care_type', '상세주소':'add'})\n",
    "\n",
    "care_pu = care_pu.iloc[:, :7]\n",
    "columns_to_drop = ['시도명', '정원수', '보육교직원수']\n",
    "care_pu = care_pu.drop(columns=columns_to_drop)\n",
    "care_pu = care_pu.rename(columns={'시군구명': 'gu', '어린이집명': 'day_care_name', '어린이집유형구분': 'day_care_type', '소재지도로명주소':'add'})\n",
    "new_order = ['gu', 'day_care_name', 'day_care_type', 'add']\n",
    "care_pu = care_pu[new_order]\n",
    "\n",
    "sungdong = sungdong.iloc[:, :7]\n",
    "columns_to_drop = ['어린이집코드', '운영현황', '우편번호']\n",
    "sungdong = sungdong.drop(columns=columns_to_drop)\n",
    "sungdong = sungdong.rename(columns={'시군구명': 'gu', '어린이집명': 'day_care_name', '어린이집유형': 'day_care_type', '상세주소':'add'})\n",
    "sungdong['day_care_name'] = sungdong['day_care_name'].str.replace('어린이집', '')\n",
    "\n",
    "care_se_total = pd.concat([care_se, sungdong], ignore_index=True)\n",
    "care_sub = pd.concat([care_se_total, care_pu], ignore_index=True)\n",
    "\n",
    "care_total = pd.merge(care_df, care_sub, on=['gu', 'day_care_name', 'day_care_type'], how='left')\n",
    "care_total['add'] = care_total['add'].fillna('null')\n",
    "care_total.drop(care_total.columns[4:11], axis=1, inplace=True)\n",
    "\n",
    "care_total['dong'] = care_total['add'].str.extract(r'\\((.*?)\\)', expand=False)\n",
    "care_total['dong'] = care_total['dong'].str.split(',').str[0]\n",
    "care_total['dong'] = care_total['dong'].str.extract(r'([^\\d]+)')\n",
    "\n",
    "## train, test에 병합 ##\n",
    "care_counts_dict = care_total['dong'].value_counts().to_dict()\n",
    "train_df['care_counts_count'] = train_df['dong'].map(care_counts_dict)\n",
    "test_df['care_counts_count'] = test_df['dong'].map(care_counts_dict)\n",
    "\n",
    "major_care = care_total.groupby('dong')['day_care_type'].agg(lambda x: x.value_counts().idxmax())\n",
    "train_df['major_care'] = train_df['dong'].map(major_care)\n",
    "test_df['major_care'] = test_df['dong'].map(major_care)\n",
    "###############################################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sN-dAadmnd42"
   },
   "source": [
    "# 2. 결측치 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Mcy7fzufnd42"
   },
   "outputs": [],
   "source": [
    "# 0이라고 판단되는 것들이라서 0으로 처리.\n",
    "train_df['호선'] = train_df['호선'].fillna('null')\n",
    "train_df['count'] = train_df['count'].fillna(0)\n",
    "train_df['dong_park_count'] = train_df['dong_park_count'].fillna(0)\n",
    "train_df['hos_dongs_count'] = train_df['hos_dongs_count'].fillna(0)\n",
    "train_df['aca_dongs_count'] = train_df['aca_dongs_count'].fillna(0)\n",
    "train_df['phar_counts_count'] = train_df['phar_counts_count'].fillna(0)\n",
    "train_df['mar_counts_count'] = train_df['mar_counts_count'].fillna(0)\n",
    "train_df['major_industry'] = train_df['major_industry'].fillna('null')\n",
    "train_df['care_counts_count'] = train_df['care_counts_count'].fillna(0)\n",
    "train_df['major_care'] = train_df['major_care'].fillna('null')\n",
    "\n",
    "# test_df 열 처리\n",
    "test_df['호선'] = test_df['호선'].fillna(0)\n",
    "test_df['count'] = test_df['count'].fillna(0)\n",
    "test_df['dong_park_count'] = test_df['dong_park_count'].fillna(0)\n",
    "test_df['hos_dongs_count'] = test_df['hos_dongs_count'].fillna(0)\n",
    "test_df['aca_dongs_count'] = test_df['aca_dongs_count'].fillna(0)\n",
    "test_df['phar_counts_count'] = test_df['phar_counts_count'].fillna(0)\n",
    "test_df['mar_counts_count'] = test_df['mar_counts_count'].fillna(0)\n",
    "test_df['major_industry'] = test_df['major_industry'].fillna('null')\n",
    "test_df['care_counts_count'] = test_df['care_counts_count'].fillna(0)\n",
    "test_df['major_care'] = test_df['major_care'].fillna('null')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['transaction_real_price']\n",
    "train_df = train_df.drop('transaction_real_price', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    수치형 변수 log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 특성별 피쳐\n",
    "binary_features = ['city']\n",
    "\n",
    "ordinal_features = ['count']\n",
    "\n",
    "categorical_features = ['dong','호선', 'major_industry',  'major_care']\n",
    "\n",
    "\n",
    "\n",
    "int_features = ['year_of_completion', 'floor', 'transaction_year_month_sort']\n",
    "\n",
    "pass_features = ['transaction_date']\n",
    "\n",
    "\n",
    "# floor는 int라서 따로 처리 안함.\n",
    "numeric_features = [\"exclusive_use_area\",\"care_counts_count\",\n",
    "                    \n",
    "                    \"hos_dongs_count\", \n",
    "                    \"aca_dongs_count\", \"dong_park_count\", \"phar_counts_count\", \"mar_counts_count\"]\n",
    "\n",
    "train_numeric_features = [\"exclusive_use_area\",\"care_counts_count\",\n",
    "                    \n",
    "                    \"hos_dongs_count\", \n",
    "                    \"aca_dongs_count\", \"dong_park_count\", \"phar_counts_count\", \"mar_counts_count\"]\n",
    "test_numeric_features = [\"exclusive_use_area\", \"care_counts_count\",\n",
    "                    \n",
    "                    \"hos_dongs_count\", \n",
    "                    \"aca_dongs_count\", \"dong_park_count\", \"phar_counts_count\", \"mar_counts_count\"]\n",
    "\n",
    "new_train_numeric_features =[]\n",
    "new_test_numeric_features =[]\n",
    "train_df = train_df[categorical_features + train_numeric_features + new_train_numeric_features + int_features+binary_features+ordinal_features]\n",
    "test_df = test_df[categorical_features+test_numeric_features+new_test_numeric_features+int_features+binary_features+ordinal_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i, feature_name in enumerate(numeric_features):\n",
    "\n",
    "    #로그\n",
    "    new_X_train_feature = np.log1p(train_df[feature_name])\n",
    "    new_X_test_feature = np.log1p(test_df[feature_name])\n",
    "\n",
    "    log_feature_name = f'logN{feature_name}'\n",
    "    train_df[log_feature_name] = new_X_train_feature\n",
    "    test_df[log_feature_name] = new_X_test_feature\n",
    "\n",
    "    new_train_numeric_features.extend([log_feature_name])\n",
    "    new_test_numeric_features.extend([log_feature_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1060575 entries, 0 to 1060574\n",
      "Data columns (total 23 columns):\n",
      " #   Column                       Non-Null Count    Dtype  \n",
      "---  ------                       --------------    -----  \n",
      " 0   dong                         1060575 non-null  object \n",
      " 1   호선                           1060575 non-null  object \n",
      " 2   major_industry               1060575 non-null  object \n",
      " 3   major_care                   1060575 non-null  object \n",
      " 4   exclusive_use_area           1060575 non-null  float64\n",
      " 5   care_counts_count            1060575 non-null  float64\n",
      " 6   hos_dongs_count              1060575 non-null  float64\n",
      " 7   aca_dongs_count              1060575 non-null  float64\n",
      " 8   dong_park_count              1060575 non-null  float64\n",
      " 9   phar_counts_count            1060575 non-null  float64\n",
      " 10  mar_counts_count             1060575 non-null  float64\n",
      " 11  year_of_completion           1060575 non-null  int64  \n",
      " 12  floor                        1060575 non-null  int64  \n",
      " 13  transaction_year_month_sort  1060575 non-null  int64  \n",
      " 14  city                         1060575 non-null  object \n",
      " 15  count                        1060575 non-null  float64\n",
      " 16  logNexclusive_use_area       1060575 non-null  float64\n",
      " 17  logNcare_counts_count        1060575 non-null  float64\n",
      " 18  logNhos_dongs_count          1060575 non-null  float64\n",
      " 19  logNaca_dongs_count          1060575 non-null  float64\n",
      " 20  logNdong_park_count          1060575 non-null  float64\n",
      " 21  logNphar_counts_count        1060575 non-null  float64\n",
      " 22  logNmar_counts_count         1060575 non-null  float64\n",
      "dtypes: float64(15), int64(3), object(5)\n",
      "memory usage: 194.2+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDN7nsjknd43"
   },
   "source": [
    "# 3. 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "J1YaHyo6nd43"
   },
   "outputs": [],
   "source": [
    "## train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "F8Nm4ijnnd44",
    "outputId": "9e62be03-e4b6-4902-b581-eeb421cad8da"
   },
   "outputs": [],
   "source": [
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', TargetEncoder(), categorical_features),\n",
    "        ('bin', BinaryEncoder(), binary_features),\n",
    "        ('ord', OneHotEncoder(), ordinal_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "preprocessor = Pipeline(\n",
    "    steps=[\n",
    "        (\"column\", column_transformer), \n",
    "        (\"selector\", SelectPercentile(percentile=98)),\n",
    "    ]\n",
    ")\n",
    "model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor), \n",
    "        (\"regressor\", RandomForestRegressor(\n",
    "                                            n_estimators=100,          # 트리의 개수\n",
    "                                            max_depth=None,             # 트리의 최대 깊이 (None으로 설정하면 노드가 확장될 때까지 분할)\n",
    "                                            min_samples_split=2,        # 노드를 분할하기 위한 최소 샘플 수\n",
    "                                            min_samples_leaf=1,         # 리프 노드에 필요한 최소 샘플 수\n",
    "                                            max_leaf_nodes=None,        # 리프 노드의 최대 개수 (None으로 설정하면 무제한)\n",
    "                                            min_impurity_decrease=0.0,  # 분할을 수행하기 위한 최소 불순도 감소\n",
    "                                            n_jobs=-1,                # 병렬 처리에 사용할 CPU 코어 수\n",
    "                                            random_state=42,          # 랜덤 시드\n",
    "                                            warm_start=True,           # 이전 호출에서 학습된 모델을 재사용하여 추가 학습 여부\n",
    "                                            ccp_alpha=0.0\n",
    "                                            )\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default LM CV scores:  [4658.46313811 4730.46928403 4691.70518088 4685.43981644 4885.15731136]\n",
      "Default LM CV mean = 4730.94 with std = 878.83\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "print(\"Default LM CV scores: \", np.sqrt(-1*scores))\n",
    "print(\"Default LM CV mean = %.2f\" % np.sqrt(-1*scores.mean()), \"with std = %.2f\" % np.sqrt(scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,y)\n",
    "pred_y = model.predict(test_df)\n",
    "LM_VERSION = 'D&A_RandomForest'\n",
    "# submission 화일 생성\n",
    "filename = f'LM_{LM_VERSION}_{np.sqrt(-1*scores.mean()):.2f}.csv'\n",
    "pd.DataFrame({'transaction_id':test_id, 'transaction_real_price':pred_y}).to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
